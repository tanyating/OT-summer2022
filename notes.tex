\documentclass[11pt]{article}

 \renewcommand*\familydefault{\sfdefault}
%%
%% to get Arial font as the sans serif font, uncomment following line:
%% \renewcommand{\sfdefault}{phv} % phv is the Arial font
\usepackage[sort,nocompress]{cite}
\usepackage[small,bf,up]{caption}
\renewcommand{\captionfont}{\footnotesize}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage{graphics,epsfig,graphicx,float,subfigure,color}
%\usepackage{algorithm,algorithmic}
\usepackage{amsmath,amssymb,amsbsy,amsfonts,amsthm}
\usepackage{url}
\usepackage{boxedminipage}
\usepackage[sf,bf,tiny]{titlesec}
 \usepackage[plainpages=false, colorlinks=true,
   citecolor=blue, filecolor=blue, linkcolor=blue,
   urlcolor=blue]{hyperref}

\usepackage{algorithmicx}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xspace}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{sidecap}
\usepackage{caption}
\usepackage[numbered,framed]{matlab-prettifier}
\lstset{
  style      = Matlab-editor,
}

\lstset{
basicstyle=\small\ttfamily,
numbers=left,
numbersep=5pt,
xleftmargin=20pt,
frame=tb,
framexleftmargin=20pt
}

\usepackage{float}


\lstset{
basicstyle=\small\ttfamily,
numbers=left,
numbersep=5pt,
xleftmargin=20pt,
frame=tb,
framexleftmargin=20pt
}

\renewcommand*\thelstnumber{\arabic{lstnumber}:}

\DeclareCaptionFormat{mylst}{\hrule#1#2#3}
\captionsetup[lstlisting]{format=mylst,labelfont=bf,singlelinecheck=off,labelsep=space}

\usepackage{matlab-prettifier}
\newcommand{\todo}[1]{\textcolor{red}{#1}}
% see documentation for titlesec package
% \titleformat{\section}{\large \sffamily \bfseries}
\titlelabel{\thetitle.\,\,\,}

\renewcommand{\baselinestretch}{0.994}
\newcommand{\bs}{\boldsymbol}
\newcommand{\alert}[1]{\textcolor{red}{#1}}
\newcommand{\abs}[1]{\left|#1\right|}

\setlength{\emergencystretch}{20pt}
\usepackage[utf8]{inputenc}


\begin{document}

\begin{center}
\vspace*{-2cm}

\end{center}
\vspace*{.5cm}
\begin{center}
\large \textbf{%%
Summer 2022: Optimal Transport}\\
\large \textbf{%%
Tanya Wang}\\
\end{center}

% ---------------------------------------------------------------
\section{Abstract}
Solve Optimal Transport, and add in extra degree of freedom (some transformations without cost). Eg. Match things with wrong size, orientation, and etc.

\section{Notes}
\subsection{Jun 14}
Given two distributions $\rho$ with $N$ samples $x^i$ and $\mu$ with $M$ samples $y^j$, find a transport $T$ such that $T(\rho) = \rho_T = \mu$ which also minimizes the some cost $C(X,T) = \sum_{i=1}^N c(x^i, T(x^i)) = \sum_{i=1}^N c(x^i, z^i)$. (assume $T(x^i) = z^i$)

\begin{enumerate}
    \item {\bf [Push Forward condition]}
    
    To impose $\rho_T=\mu$, there's a weak formulation requiring that the expected values of any function $F$ should be the same calculated based on $\rho_T$ and $\mu$.
    \begin{align*}
        \mathbb{E}_{\rho_{T}} [F(\cdot)] = \mathbb{E}_{\mu} [F(\cdot)]\\
        \int F(x) \rho_T(x) dx = \int F(y) \mu(y) dy
    \end{align*}
    Since the distributions are unknown, rather we are only given with samples drawn from the 2 populations, we can only enforce the sample mean to be equal, ie. $\frac{1}{N} \sum_i F(z^i) = \frac{1}{M} \sum_j F(y^j)$.
    
    \item {\bf [Minimization with constraint]}
    
    To minimize the cost while maintaining the push forward condition, this becomes an optimization problem with constraints and the {\bf Lagrange function} depending on $T$ and $F$ is 
    \begin{align}
        L = C(X,T) - (\mathbb{E}_{\rho_{T}} [F(\cdot)] - \mathbb{E}_{\mu} [F(\cdot)])
    \end{align}
    So now we have a minmax problem: $\min_T \max_F L$.
    
    \item {\bf [Test function $F$]}
    
    Assume $F$ belongs to a family of functions that do not oversolve depending on the samples/data. Specifically, we can pick a nice, fixed $F$
    \begin{align}
        F(y) = \rho_T(y) - \mu(y)
    \end{align}
    And plug it in, we get the new Lagrange function (assume $\lambda>0$):
    \begin{align*}
        L = C(X,T) + \lambda \int (\rho_T - \mu)^2(y) dy
    \end{align*}
    where our minmax problem becomes $\min_T \max_\lambda L$ (can consider $\lambda$ as a regularization/penalty parameter, when $\lambda$ is large, $\rho_T(\cdot) = \mu(\cdot)$ has to be achieved.)
    
    \item {\bf [Minimization problem]}
    
    For some fixed, large $\lambda$, and given test function $F$, the problem becomes
    \begin{align}
    \label{eq3}
        \min_T L 
        = \min_T \left[C(X,T) + \lambda (\mathbb{E}_{\rho_{T}} [F(\cdot)] - \mathbb{E}_{\mu} [F(\cdot)]) \right]\\
        = \min_T \left[C(X,T) + \lambda (\int F(x)\rho_T(x) dx - \int F(y)\mu(y) dy) \right]
    \end{align}
    And we can replace the expected values with samples means:
    $$
    \mathbb{E}_{\rho_{T}} [F(\cdot)] \approx \frac{1}{N} \sum_i F(T(x^i))
    = \frac{1}{N} \sum_i F(z^i)
    $$
    $$
    \mathbb{E}_{\mu} [F(\cdot)] \approx \frac{1}{M} \sum_j F(y^j)
    $$
    So the only thing remain unknown is how to compute $F(y) = \rho_T(y) - \mu(y)$, which turns into how to compute the densities $\rho_T(\cdot)$ and $\mu(\cdot)$.
    
    \item {\bf [Kernel Density Estimation (KDE)]}
    
    Given samples $y^j$ (or $z^i$), we want to know the expression of the probability density $\mu(y^j)$ (or $\rho_T(z^i)$). KDE uses a sum of kernel functions to estimate the density (non-parametric way).
    \begin{align*}
        \hat{\mu}_a(y) = \frac{1}{M} \sum_j \kappa_a(y,y^j)
        = \frac{1}{aM} \sum_j \kappa(\frac{y-y_j}{a})
    \end{align*}
    where $M$ is the number of samples (data), $a$ is bandwidth, and $\kappa$ is the kernel (eg. uniform, normal, etc.).
    
    Bandwidth selection:
    \begin{enumerate}
        \item (Deep)
        
        Use samples $y^j$ to write out the formula for $\hat{\mu}_a(y^j)$, and then do max (log)likelihood over the free parameter $a$. Note the original formula for $\hat{\mu}_a(y^j)$ would result in an optimizer $a=0$, so use instead $\hat{\mu}_a(y^j) = \frac{1}{M-1} \sum_{i\ne j} \kappa_a(y^j,y^i)$ to optimize over the sum of log likelihood.
        
        \item (Simple, common Rule of Thumb)
        
        Assume the target distribution $\mu$ is Gaussian (and the kernel basis functions are also Gaussian), then do max likelihood analytically over $a$. (ref: \href{https://en.wikipedia.org/wiki/Kernel_density_estimation}{wikipedia})
        \begin{align*}
            a = (\frac{4\hat{\sigma}^5}{3M})^{\frac{1}{5}} \approx
            1.06 \hat{\sigma} M^{-\frac{1}{5}}
        \end{align*}
        where $\hat{\sigma} = \min (\text{standard deviation}, \frac{IQR}{1.34})$.
        
        Another modification that will improve the model is to reduce the factor from 1.06 to 0.9. Then the final formula would be:
        \begin{align*}
            a = 0.9 \min (\hat{\sigma}, \frac{IQR}{1.34}) M^{-\frac{1}{5}}
        \end{align*}
        where $\hat{\sigma}$ now becomes the standard deviation of the samples.
    \end{enumerate}
    
    \item {\bf [Gradient Descent]}
    
    With KDE, we now go back to solve the minimization problem (\ref{eq3}).
    We see that the transport $T$ depends only on the transported samples $z^j$. So we can simply do a gradient descent in the direction of $-\nabla_{\mathbf{z}} L$ with some learning rate $\eta$, where $\mathbf{z} = \begin{pmatrix}
    z^1 \\ \vdots \\ z^N
    \end{pmatrix}$.
    
        \begin{algorithm}[h]
        \caption{Gradient Descent w.r.t $\mathbf{z}$}
        \begin{algorithmic}
        \For{$i = 1, \dots, N$}
        \State $z^i \gets x^i$
        \Comment{Start with the original data points}
        \EndFor
            
        \While{not reaching max steps or cost still large}
        \textcolor{blue}{\State $\mathbf{z} \gets \mathbf{z} - \eta \nabla_{\mathbf{z}} L$} \Comment{Gradient descent}
        \EndWhile
            
        \end{algorithmic}
        \end{algorithm}
    
    \item {\bf [Gaussian Kernel basis functions]}
    
    For KDE, use the default standard normal distribution as kernel, then the bandwidth becomes the standard deviation.
    ie. $\kappa_a(y,y^j) = \frac{1}{a \sqrt{2\pi}} e^{-\frac{1}{2}(\frac{y-y^j}{a})^2}$. Now we split the Lagrange/loss function into 2 parts and take gradient wrt $\mathbf{z}$. (\textcolor{purple}{Note: we also assume in 1D?})
    $$
    L = C(X,T) + \lambda (\mathbb{E}_{\rho_{T}} [F(\cdot)] - \mathbb{E}_{\mu} [F(\cdot)])
    $$
    $$
    = \textcolor{blue}{C(X,T)} + \textcolor{red}{\lambda (\frac{1}{N} \sum_i F(z^i) - \frac{1}{N} \sum_i F(y^i))}
    $$
    \begin{align}
        = \textcolor{blue}{L_C} + \textcolor{red}{L_F}
    \end{align}
    
    \begin{enumerate}
        \item 
        $$
        \textcolor{blue}{L_C} = \int c(x,T(x)) \rho(x) dx
        $$
        $$
        \approx \frac{1}{N} \sum_i c(x^i,T(x^i))
        = \frac{1}{N} \sum_i c(x^i,z^i)
        $$
        where $c(x,y)$ is some cost function. Specifically, we can take the most common Euclidean distance: $c(x,y) = \frac{1}{2} \|x-y\|^2$.
        $$
        \textcolor{blue}{L_C} = \frac{1}{N} \sum_i \frac{1}{2}(x^i-z^i)^2
        $$
        \begin{align*}
            \implies \frac{\partial \textcolor{blue}{L_C}}{\partial z^i}
            = - \frac{1}{N}(x^i-z^i)
        \end{align*}
        
        \item Plug in KDE into $\textcolor{red}{L_F}$ (assume $a$ as the bandwidth for $\hat{\rho_T}$ and $b$ as the bandwidth for $\hat{\mu}$.):
        \begin{align*}
        % \centering
            \textcolor{red}{L_F} = \lambda (\frac{1}{N} \sum_i F(z^i) - \frac{1}{M} \sum_i F(y^i)\\
            =  \lambda [\frac{1}{N} \sum_i (\frac{1}{N} \sum_j \kappa_a(z^i,z^j) - \frac{1}{M} \sum_j \kappa_a(z^i,y^j))\\
            - \frac{1}{M} \sum_i (\frac{1}{N} \sum_j \kappa_a(y^i,z^j) - \frac{1}{M} \sum_j \kappa_a(y^i,y^j))]\\
            = \lambda \left[\sum_i \sum_j \left( 
            \frac{1}{N^2}\kappa_a(z^i,z^j) -  \frac{1}{MN}\kappa_b(z^i,y^j) -   \frac{1}{MN}\kappa_a(y^i,z^j) +  \frac{1}{M^2}\kappa_b(y^i,y^j)
            \right) \right]
        \end{align*}
        
        \begin{align*}
            \implies \frac{\partial \textcolor{red}{L_F}}{\partial z^i}
            = \lambda \sum_j \left[
             \frac{1}{N^2}\frac{2}{a^3\sqrt{2\pi}} (z^j-z^i) e^{-\frac{1}{2}(\frac{z^j-z^i}{a})^2} -  \frac{1}{MN}\frac{1}{b^3\sqrt{2\pi}} (y^j-z^i) e^{-\frac{1}{2}(\frac{y^j-z^i}{b})^2} \right]
        \end{align*}
        Note we don't take partial derivative of $z^i$ wrt the KDE using $z^i$ as the center of the kernel (ie. we don't consider $\frac{\partial \kappa_a(y^i,z^j)}{\partial z^i}$).
        \end{enumerate}
    
    So to do gradient descent, we have $(-\nabla_{\mathbf{z}} L)_i =  -\frac{\partial L}{\partial z^i} = -  \frac{\partial \textcolor{blue}{L_C}}{\partial z^i} -  \frac{\partial \textcolor{red}{L_F}}{\partial z^i}$.
    
\end{enumerate}

\end{document}